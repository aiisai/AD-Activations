# AD-Activations
#### Artificial Derivative Activation Functions:
> AD ReLU1
> AD ReLU2
> AD Sigmoid1
> AD Sigmoid2
#### (based on *"Handling Vanishing Gradient Problem Using Artificial Derivative"* with some extension)
#### Please cite this paper:
##### *Z. Hu, J. ZHANG and Y. Ge, "Handling Vanishing Gradient Problem Using Artificial Derivative," in IEEE Access, doi: 10.1109/ACCESS.2021.3054915.*
#### Good luck :)
